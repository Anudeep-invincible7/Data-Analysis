{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I144d0BB6h35"
   },
   "source": [
    "## 1. Questions generation based on nouns and verbs..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkR9NoX3VwNV"
   },
   "source": [
    "This notebook has been implemented on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vwk_xR3l6gdf"
   },
   "outputs": [],
   "source": [
    "##### Sample input text is given here. Please change the text input as per your requirement accordingly to generate questions.\n",
    "##text = \"\"\"Socrates was born in 470 or 469 BC to Sophroniscus and Phaenarete, a stoneworker and a midwife, respectively, in the Athenian deme of Alopece; therefore, he was an Athenian citizen, having been born to relatively affluent Athenians. He lived close to his father's relatives and inherited, as was customary, part of his father's estate, securing a life reasonably free of financial concerns. His education followed the laws and customs of Athens. He learned the basic skills of reading and writing and, like most wealthy Athenians, received extra lessons in various other fields such as gymnastics, poetry and music. He was married twice (which came first is not clear): his marriage to Xanthippe took place when Socrates was in his fifties, and another marriage was with a daughter of Aristides, an Athenian statesman.”\"\"\"\n",
    "##text = \"\"\"It is likely that humans and dogs have shared a special bond of friendship and mutual support ever since at least the Neolithic period — but why has this bond been so long-lasting? Of course, these cousins of the wolves have historically been great at keeping us and our dwellings safe, guarding our houses, our cattle, and our various material goods. Throughout history, humans have also trained dogs to assist them with hunting, or they have bred numerous quirky-looking species for their cuteness or elegance.However, dogs are also — and might have always been — truly valued companions, famed for their loyalty and seemingly constant willingness to put a smile on their owners’ faces.In this Spotlight, we outline the research that shows how our dogs make us happier, more resilient when facing stress, and physically healthier, to name but a few ways in which these much-loved quadrupeds support our well-being.\"\"\"\n",
    "text = \"\"\"The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism). Rowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the internet, their fave is problematic. This fan-creator relationship is a peculiarly modern one, a mixture of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines. (Dahl had a history of making anti-Semitic statements, and Blyton’s books are studies in casual racism.) They didn’t live long enough to see complaints about the whiteness and straightness of their books, or to upset their readers with unguarded tweets.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgYeOwd8Qbki"
   },
   "source": [
    "The above random text has taken from the source : https://www.theatlantic.com/international/archive/2020/07/why-millennial-harry-potter-fans-reject-jk-rowling/613870/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0GpMMllApOt"
   },
   "source": [
    "## Installation of all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wUYrFx7P0JVy"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet git+https://github.com/boudinfl/pke.git@5af1f817e0211c33ac3f90e1e86bb5c1283448e8\n",
    "!pip install --quiet flashtext==2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_pfSKhD0Z1z",
    "outputId": "f2d85854-bed4-45cd-90da-3d11b42c6ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-08 07:55:04.482813: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 9.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "2022-09-08 07:55:22.137676: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.4.1                         \n",
      "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
      "Platform         Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
      "Python version   3.7.13                        \n",
      "Pipelines        en_core_web_sm (3.4.0)        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dYS-YoSPcj3s"
   },
   "outputs": [],
   "source": [
    "# This cell is highlighting the commas,periods and other puntuations presented in the sentences\n",
    "#loaded english webtext pipeling to process the text\n",
    "# !pip install spacy==2.2.4\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# text_spacy = text\n",
    "# doc = nlp(text_spacy)\n",
    "# a=list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crT89xax0g1l",
    "outputId": "72ff9d99-fe7c-4f92-e609-fa8189ed587f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZyARWvqdIo2",
    "outputId": "dc6ce799-a0be-49be-d1f8-742b3958efc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in\n",
      "the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism). Rowling’s newer work in the Potter universe reminds them, however,\n",
      "that none of this is canon—that, in the language of the internet, their fave is problematic. This fan-creator relationship is a peculiarly modern one, a mixture of entitlement\n",
      "and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines. (Dahl had a history of\n",
      "making anti-Semitic statements, and Blyton’s books are studies in casual racism.) They didn’t live long enough to see complaints about the whiteness and straightness of their\n",
      "books, or to upset their readers with unguarded tweets.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "text_wrap = textwrap.TextWrapper(width=175)\n",
    "sent_list = text_wrap.wrap(text=text)\n",
    "wrapped=[print(sentences) for sentences in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWSQM-tU0yRi",
    "outputId": "cf1c44c2-fa11-4ad0-959e-ec477aec5a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['harry potter', 'fan fiction', 'millennial', 'own values', 'nostalgic setting', 'british private-school system', 'perpetuate elitism', 'rowling', '’ s', 'potter', 'universe reminds', 'fan-creator relationship', 'enid blyton', 'roald dahl', 'consumer revolts', 'personal opinions', 'dahl', 'anti-semitic statements', 'blyton', '’ s books', 'casual racism', 'didn ’ t'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Extracting all the noun phrases present in the text by using text blob library.\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(text)\n",
    "noun_phrases=blob.noun_phrases\n",
    "noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qMqzqZ6fKhN",
    "outputId": "4d1b5857-3f95-4af4-c739-4a3a0f671007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Installing pke library for the python keyword extraction from external source github https://github.com/boudinfl/pke\n",
    "# we have imported all the necessary libraries like string,re,nltk, stopwords, wordnet, pke and word_tokenizer\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import traceback\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bomfUJKQcWAq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIdwVViamBHC",
    "outputId": "9cd13678-e56f-43a6-dc8e-b204e8e1a5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "159\n",
      "230\n",
      "104\n",
      "148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism).',\n",
       " 'Rowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the internet, their fave is problematic.',\n",
       " 'This fan-creator relationship is a peculiarly modern one, a mixture of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines.',\n",
       " '(Dahl had a history of making anti-Semitic statements, and Blyton’s books are studies in casual racism.)',\n",
       " 'They didn’t live long enough to see complaints about the whiteness and straightness of their books, or to upset their readers with unguarded tweets.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Excluding the sentences which are having length less than 30\n",
    "## So that our creation of question wont confuse the readers if they are big and understandable meaningful sentences\n",
    "sent_list = text_wrap.wrap(text=text)\n",
    "sent_list=sent_tokenize(text)\n",
    "main_sent=[]\n",
    "for sentences in sent_list:\n",
    "    print(len(sentences))\n",
    "    if len(sentences) > 30:\n",
    "        main_sent.append(sentences.strip())\n",
    "main_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChAZN_fzBCti"
   },
   "source": [
    "## Extraction of Keywords using position tag of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLax3AsIGdtX",
    "outputId": "6fa4e657-926c-4195-9f48-e1a61625a692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words which are verbs present in the text (with duplicate elements):  ['allowed', 'rewrite', 'easing', 'luxuriating', 'designed', 'perpetuate', 'Rowling', 'reminds', 'troubled', 'making']\n",
      "Words which are Nouns present in the text (with duplicate elements):  ['Potter', 'fiction', 'audience', 'stories', 'values', 'discomfort', 'setting', 'private-school', 'system', 'institution', 'elitism', 'Potter', 'language', 'internet', 'relationship', 'mixture', 'entitlement', 'intimacy', 'lifetimes', 'Blyton', 'consumer', 'revolts', 'opinions', 'plotlines', 'history', 'statements', 'Blyton', 'studies', 'racism', 'complaints', 'whiteness', 'straightness', 'readers', 'tweets']\n",
      "Words which are verbs present in the text:  ['allowed', 'rewrite', 'easing', 'luxuriating', 'designed', 'perpetuate', 'Rowling', 'reminds', 'troubled', 'making']\n",
      "Words which are Nouns present in the text:  ['Potter', 'fiction', 'audience', 'stories', 'values', 'discomfort', 'setting', 'private-school', 'system', 'institution', 'elitism', 'language', 'internet', 'relationship', 'mixture', 'entitlement', 'intimacy', 'lifetimes', 'Blyton', 'consumer', 'revolts', 'opinions', 'plotlines', 'history', 'statements', 'studies', 'racism', 'complaints', 'whiteness', 'straightness', 'readers', 'tweets']\n"
     ]
    }
   ],
   "source": [
    "#Extracting all the nouns and verbs present in the text using nltk,pos_tag \n",
    "wrd_tokenize=nltk.word_tokenize(text)\n",
    "position_parts_of_speech=nltk.pos_tag(wrd_tokenize)\n",
    "verb_words_with_duplicates=[]\n",
    "noun_words_with_duplicates=[]\n",
    "for (word,pos) in position_parts_of_speech:\n",
    "    if pos[0]== 'V': ##Extracting key words of verbs\n",
    "        if(len(word)>5):\n",
    "            verb_words_with_duplicates.append(word)\n",
    "    if pos[0]=='N':  ##Extracting key words of Nouns\n",
    "        if(len(word)>5):\n",
    "            noun_words_with_duplicates.append(word)\n",
    "verb_words=[]\n",
    "noun_words=[]\n",
    "for words in verb_words_with_duplicates:\n",
    "    if words not in verb_words:\n",
    "        verb_words.append(words)\n",
    "for words in noun_words_with_duplicates:\n",
    "    if words not in noun_words:\n",
    "        noun_words.append(words)\n",
    "print('Words which are verbs present in the text (with duplicate elements): ',verb_words_with_duplicates)\n",
    "print('Words which are Nouns present in the text (with duplicate elements): ',noun_words_with_duplicates)\n",
    "print('Words which are verbs present in the text: ',verb_words)\n",
    "print('Words which are Nouns present in the text: ',noun_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxQE7pPPBRNq"
   },
   "source": [
    "## Extraction of Keywords using Python keyword extractor (PKE) Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "UzscglVaiHZL",
    "outputId": "e6bc7452-0e86-4514-bcb5-135477f61062"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism). Rowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the internet, their fave is problematic. This fan-creator relationship is a peculiarly modern one, a mixture of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines. (Dahl had a history of making anti-Semitic statements, and Blyton’s books are studies in casual racism.) They didn’t live long enough to see complaints about the whiteness and straightness of their books, or to upset their readers with unguarded tweets.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPlXWNaTNtTk",
    "outputId": "77edbb70-de93-4ed2-d14b-d55f5cdc19fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyphrases in the text:  ['books', 'millennial audience', 'rewrite', 'stories', 'entitlement', 'allowed', 'mixture', 'problematic', 'fave', 'intimacy', 'studies', 'fit', 'consumer revolts', 'discomfort', 'easing', 'fan-creator relationship', 'making anti-semitic statements', 'institution designed', 'readers', 'personal opinions', 'internet', 'nostalgic setting', 'casual racism', 'british private-school system', 'fan fiction', 'straightness', 'modern', 'troubled', 'history', 'luxuriating']\n"
     ]
    }
   ],
   "source": [
    "# ### using istalled pke library from the git\n",
    "# ### I am extracting only the words of part speech belongs to Verb, Adjective and Noun\n",
    "# ### Creating stop words as they dont add much weightage\n",
    "def pos_words_generation(text):\n",
    "    out=[]    \n",
    "    stop_list_punct_duplicates = []\n",
    "    stop_list_punct= []\n",
    "    for words in nltk.word_tokenize(text):\n",
    "        if words in list(string.punctuation):\n",
    "            stop_list_punct_duplicates.append(words)\n",
    "    for w in stop_list_punct_duplicates:\n",
    "        if w not in stop_list_punct:\n",
    "            stop_list_punct.append(w)\n",
    "    pos_word_getter = pke.unsupervised.MultipartiteRank()\n",
    "    pos_word_getter.load_document(input=text)\n",
    "    brackets=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stop_list1=brackets\n",
    "    stop_list2=stopwords.words('english')\n",
    "    stop_list=stop_list_punct + stop_list1 + stop_list2\n",
    "    position_vector = {'VERB','ADJ','NOUN'}\n",
    "    pos_word_getter.candidate_selection(pos=position_vector)\n",
    "    pos_word_getter.candidate_weighting(alpha=1.1,threshold=0.75,method='average')\n",
    "    keyphrases = pos_word_getter.get_n_best(n=30)\n",
    "    for val in keyphrases:\n",
    "        out.append(val[0])\n",
    "\n",
    "    if len(out)==0:\n",
    "        print('No words for given Parts of speech present in the text!')\n",
    "\n",
    "    return out\n",
    "\n",
    "sent_V_A_N = pos_words_generation(text)\n",
    "print('keyphrases in the text: ',sent_V_A_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fxP0g9O_Odv"
   },
   "source": [
    "#### We are comparing both the words generated by PKE library and pos_tag of NLTK. Either it is Verb (V) or Noun (N) as we give below, Our code generates the set of keywords only if they are present in the both the result sets to gain more accuracy.\n",
    "\n",
    "\n",
    "\n",
    "**Please provide the option of whether the generation of keywords based on Nouns or Verbs,present in the each sentence of the paragraph, where the comment is provided.Give input as 'N' for Noun or 'V' for verb...** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jygXoZqdCrvA",
    "outputId": "8d1baf29-5cb8-4b8c-d23b-0b0c208db108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0:\n",
      " - PoS: DET NOUN ADP PROPN PROPN NOUN NOUN AUX VERB PRON ADJ NOUN PART VERB DET NOUN PART VERB PRON ADJ NOUN PUNCT VERB PRON NOUN PUNCT SCONJ ADV VERB ADP DET ADJ NOUN ADP DET ADJ NOUN NOUN PUNCT DET NOUN VERB PART VERB NOUN PUNCT PUNCT\n",
      "sentence 1:\n",
      " - PoS: VERB NOUN ADJ NOUN ADP DET PROPN NOUN VERB PRON PUNCT ADV PUNCT SCONJ NOUN ADP DET AUX ADJ PUNCT ADP DET NOUN ADP DET NOUN PUNCT PRON NOUN AUX ADJ PUNCT\n",
      "sentence 2:\n",
      " - PoS: DET NOUN NOUN AUX DET ADV ADJ NUM PUNCT DET NOUN ADP NOUN CCONJ NOUN PUNCT ADP PRON NOUN PUNCT PROPN PROPN CCONJ PROPN PROPN AUX PART VERB ADP NOUN NOUN ADP PRON ADJ NOUN CCONJ PRON NOUN PUNCT PUNCT\n",
      "sentence 3:\n",
      " - PoS: PROPN AUX DET NOUN ADP VERB ADJ NOUN PUNCT CCONJ PROPN PART NOUN AUX NOUN ADP ADJ NOUN PUNCT PUNCT\n",
      "sentence 4:\n",
      " - PoS: PRON AUX PART VERB ADV ADV PART VERB NOUN ADP DET NOUN CCONJ NOUN ADP PRON NOUN PUNCT CCONJ PART VERB PRON NOUN ADP ADJ NOUN PUNCT\n"
     ]
    }
   ],
   "source": [
    "word_pos = pke.unsupervised.TopicRank()\n",
    "word_pos.load_document(input=text, language='en')\n",
    "for i, sentence in enumerate(word_pos.sentences):   \n",
    "    # print out the sentence id, its tokens, its stems and the corresponding Part-of-Speech tags\n",
    "    print(\"sentence {}:\".format(i))\n",
    "    print(\" - PoS: {}\".format(' '.join(sentence.pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rj7UFzzN-Wb",
    "outputId": "f57667ad-812b-43ac-c2ae-5a1e54dd9249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stories', 'discomfort', 'internet', 'mixture', 'entitlement', 'intimacy', 'history', 'studies', 'straightness', 'readers']\n",
      "{'discomfort': ['The acres of Harry Potter fan fiction have allowed its '\n",
      "                'Millennial audience to rewrite the stories to fit their own '\n",
      "                'values, easing their discomfort (while still luxuriating in '\n",
      "                'the nostalgic setting of the British private-school system, '\n",
      "                'an institution designed to perpetuate elitism).'],\n",
      " 'entitlement': ['This fan-creator relationship is a peculiarly modern one, a '\n",
      "                 'mixture of entitlement and intimacy: In their lifetimes, '\n",
      "                 'Enid Blyton and Roald Dahl were not troubled by consumer '\n",
      "                 'revolts over their personal opinions or their plotlines.'],\n",
      " 'history': ['(Dahl had a history of making anti-Semitic statements, and '\n",
      "             'Blyton’s books are studies in casual racism.)'],\n",
      " 'internet': ['Rowling’s newer work in the Potter universe reminds them, '\n",
      "              'however, that none of this is canon—that, in the language of '\n",
      "              'the internet, their fave is problematic.'],\n",
      " 'intimacy': ['This fan-creator relationship is a peculiarly modern one, a '\n",
      "              'mixture of entitlement and intimacy: In their lifetimes, Enid '\n",
      "              'Blyton and Roald Dahl were not troubled by consumer revolts '\n",
      "              'over their personal opinions or their plotlines.'],\n",
      " 'mixture': ['This fan-creator relationship is a peculiarly modern one, a '\n",
      "             'mixture of entitlement and intimacy: In their lifetimes, Enid '\n",
      "             'Blyton and Roald Dahl were not troubled by consumer revolts over '\n",
      "             'their personal opinions or their plotlines.'],\n",
      " 'readers': ['They didn’t live long enough to see complaints about the '\n",
      "             'whiteness and straightness of their books, or to upset their '\n",
      "             'readers with unguarded tweets.'],\n",
      " 'stories': ['The acres of Harry Potter fan fiction have allowed its '\n",
      "             'Millennial audience to rewrite the stories to fit their own '\n",
      "             'values, easing their discomfort (while still luxuriating in the '\n",
      "             'nostalgic setting of the British private-school system, an '\n",
      "             'institution designed to perpetuate elitism).'],\n",
      " 'straightness': ['They didn’t live long enough to see complaints about the '\n",
      "                  'whiteness and straightness of their books, or to upset '\n",
      "                  'their readers with unguarded tweets.'],\n",
      " 'studies': ['(Dahl had a history of making anti-Semitic statements, and '\n",
      "             'Blyton’s books are studies in casual racism.)']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This cell will extract all the sentences which are containing the keyphrases we generated in the above cell.\n",
    "#importing pprint for the sentences to be arranged in easily readable way.\n",
    "#Sorting the sentences in a way that longer length first then shorter length next so that they can be easily readable.\n",
    "#we are trying to take the common words in the list of words we extracted using both nltk.pos_tag and pke based on the parts of speech nouns and verbs\n",
    "\n",
    "keyword_process = KeywordProcessor()\n",
    "sentences_with_keyword = {}\n",
    "keyword_text=[]\n",
    "\n",
    "from pprint import pprint\n",
    "def Parts_of_speech_words(wordlist_nouns,wordlist_verbs,main,pos):\n",
    "    keyword_text_processed=[]\n",
    "\n",
    "    if pos=='N':\n",
    "         for words_n in wordlist_nouns:\n",
    "                for words in main:\n",
    "                      if (words_n == words):\n",
    "                             keyword_text_processed.append(words_n)\n",
    "\n",
    "    if pos=='V':\n",
    "         for words_v in wordlist_verbs:\n",
    "                for words in main:\n",
    "                      if (words_v == words):\n",
    "                            keyword_text_processed.append(words_v)\n",
    "    return keyword_text_processed\n",
    "\n",
    "keyword_text=Parts_of_speech_words(noun_words,verb_words,sent_V_A_N,'N') #<--------------------- Are we framing questions with Nouns or Verbs, if nouns give 'N' else 'V'\n",
    "#keyword_text=Parts_of_speech_words(noun_words,verb_words,sent_V_A_N,'V')\n",
    "print(keyword_text)\n",
    "\n",
    "for word in keyword_text:\n",
    "    if len(word)>4: #not passing small words, it would make testing system too predictable if it has small meaning\n",
    "              sentences_with_keyword[word]=[]\n",
    "              keyword_process.add_keyword(word)\n",
    "for lines in main_sent:  \n",
    "    present_keywords=keyword_process.extract_keywords(lines)\n",
    "    \n",
    "    for key in present_keywords:\n",
    "        sentences_with_keyword[key].append(lines) \n",
    "pprint (sentences_with_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsBj1LbctgQi",
    "outputId": "08da1bf8-7673-4f78-c66a-e66624c18502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stories\n",
      "discomfort\n",
      "internet\n",
      "mixture\n",
      "entitlement\n",
      "intimacy\n",
      "history\n",
      "studies\n",
      "straightness\n",
      "readers\n",
      "{'keys': ['stories', 'internet', 'mixture', 'history', 'straightness'],\n",
      " 'sentences': ['The acres of Harry Potter fan fiction have allowed its '\n",
      "               'Millennial audience to rewrite the ..................  to fit '\n",
      "               'their own values, easing their discomfort (while still '\n",
      "               'luxuriating in the nostalgic setting of the British '\n",
      "               'private-school system, an institution designed to perpetuate '\n",
      "               'elitism).',\n",
      "               'Rowling’s newer work in the Potter universe reminds them, '\n",
      "               'however, that none of this is canon—that, in the language of '\n",
      "               'the .................. , their fave is problematic.',\n",
      "               'This fan-creator relationship is a peculiarly modern one, a '\n",
      "               '..................  of entitlement and intimacy: In their '\n",
      "               'lifetimes, Enid Blyton and Roald Dahl were not troubled by '\n",
      "               'consumer revolts over their personal opinions or their '\n",
      "               'plotlines.',\n",
      "               '(Dahl had a ..................  of making anti-Semitic '\n",
      "               'statements, and Blyton’s books are studies in casual racism.)',\n",
      "               'They didn’t live long enough to see complaints about the '\n",
      "               'whiteness and ..................  of their books, or to upset '\n",
      "               'their readers with unguarded tweets.'],\n",
      " 'title': 'Choose a right word for the missing part of the sentences'}\n"
     ]
    }
   ],
   "source": [
    "def pick_right_word(MatchedSent):\n",
    "    ques_generation={\"title\":\"Choose a right word for the missing part of the sentences\"}\n",
    "    blank_sentences = []\n",
    "    formed_sentences = []\n",
    "    keys=[]\n",
    "    formed_sentences_with_answers=[]\n",
    "    for key in MatchedSent.keys():\n",
    "        print(key)\n",
    "        if len(MatchedSent[key])>0:\n",
    "                length=MatchedSent[key]\n",
    "                length=sorted(length,key=len,reverse=True)\n",
    "                MatchedSent[key]=length\n",
    "\n",
    " #### This code can replace even if 2 keywords present in the same sentence \n",
    " #### It will test the student better if 2 cloze questions present in the same sentence of the paragraph.       \n",
    "    for key in MatchedSent:\n",
    "            main_sentence = MatchedSent[key][0]\n",
    "            Top_sentence = main_sentence\n",
    "            blank_sent = re.compile(re.escape(key), re.IGNORECASE)\n",
    "            count_word_per_sentence =  len(re.findall(re.escape(key),Top_sentence,re.IGNORECASE))\n",
    "            line = blank_sent.sub('.................. ', Top_sentence)\n",
    "            if (MatchedSent[key][0] not in formed_sentences) and count_word_per_sentence<3:\n",
    "                formed_sentences_with_answers.append(MatchedSent[key][0]) ###appending all the  sentences which are generating questions in fill_in_the_blanks\n",
    "\n",
    "                if (count_word_per_sentence==2):\n",
    "                     continue\n",
    "                  \n",
    "                blank_sentences.append(line)\n",
    "                formed_sentences.append(MatchedSent[key][0])\n",
    "                keys.append(key)\n",
    "                \n",
    "    ques_generation[\"sentences\"]=blank_sentences[:10]\n",
    "    ques_generation[\"keys\"]=keys[:10]\n",
    "    return ques_generation,formed_sentences_with_answers\n",
    "fill_in_the_blanks,formed_sentences_with_answers = pick_right_word(sentences_with_keyword)\n",
    "pprint(fill_in_the_blanks)\n",
    "final_sentences=[]\n",
    "for sent in formed_sentences_with_answers:\n",
    "    if sent not in final_sentences:\n",
    "        final_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMifkMb6CIPK"
   },
   "source": [
    "## Shuffling of the answers and presenting the question format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcNPsZB_uTe3",
    "outputId": "4c8d439a-1e22-455c-fe61-c8556171dd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mPick the right word for the below sentences where the space is provided (Note: All the below questions are extracted from same context): \u001b[0m \u001b[1m\u001b[34m['straightness', 'stories', 'internet', 'mixture', 'history']\u001b[0m\n",
      "\n",
      "1. The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the ..................  to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism).\n",
      "\n",
      "2. Rowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the .................. , their fave is problematic.\n",
      "\n",
      "3. This fan-creator relationship is a peculiarly modern one, a ..................  of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines.\n",
      "\n",
      "4. (Dahl had a ..................  of making anti-Semitic statements, and Blyton’s books are studies in casual racism.)\n",
      "\n",
      "5. They didn’t live long enough to see complaints about the whiteness and ..................  of their books, or to upset their readers with unguarded tweets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from termcolor import colored\n",
    "shuffle_list=[]\n",
    "for words in fill_in_the_blanks['keys']: ###shuffling and reshuffling to avoid same sequence of answers appear as per the blanks provided\n",
    "    shuffle_list.append(words)\n",
    "if random.shuffle(shuffle_list)==shuffle_list:\n",
    "    random.shuffle(shuffle_list)\n",
    "heading=colored('Pick the right word for the below sentences where the space is provided (Note: All the below questions are extracted from same context): ','red',attrs=['bold'])\n",
    "shuffle_color = colored(shuffle_list,'blue',attrs=['bold'])\n",
    "print(heading,'{}\\n'.format(shuffle_color))\n",
    "for a, b in enumerate((s for s in fill_in_the_blanks['sentences']), 1):\n",
    "    print('{}. {}\\n'.format(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX5L2DT2DiYi"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7dIO39nDne2",
    "outputId": "232ccbb3-e9d8-4a1c-c296-33c48744b7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate 0: acr (stemmed form)\n",
      "--> Main word: ['acres']\n",
      "offsets position: [1]\n",
      "Position of the sentence: [0]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 3: stori (stemmed form)\n",
      "--> Main word: ['stories']\n",
      "offsets position: [15]\n",
      "Position of the sentence: [0]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 4: discomfort (stemmed form)\n",
      "--> Main word: ['discomfort']\n",
      "offsets position: [24]\n",
      "Position of the sentence: [0]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 7: institut (stemmed form)\n",
      "--> Main word: ['institution']\n",
      "offsets position: [40]\n",
      "Position of the sentence: [0]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 8: elit (stemmed form)\n",
      "--> Main word: ['elitism']\n",
      "offsets position: [44]\n",
      "Position of the sentence: [0]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 10: languag (stemmed form)\n",
      "--> Main word: ['language']\n",
      "offsets position: [69]\n",
      "Position of the sentence: [1]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 11: internet (stemmed form)\n",
      "--> Main word: ['internet']\n",
      "offsets position: [72]\n",
      "Position of the sentence: [1]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 12: fave (stemmed form)\n",
      "--> Main word: ['fave']\n",
      "offsets position: [75]\n",
      "Position of the sentence: [1]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 16: mixtur (stemmed form)\n",
      "--> Main word: ['mixture']\n",
      "offsets position: [89]\n",
      "Position of the sentence: [2]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 17: entitl (stemmed form)\n",
      "--> Main word: ['entitlement']\n",
      "offsets position: [91]\n",
      "Position of the sentence: [2]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 18: intimaci (stemmed form)\n",
      "--> Main word: ['intimacy']\n",
      "offsets position: [93]\n",
      "Position of the sentence: [2]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 19: lifetim (stemmed form)\n",
      "--> Main word: ['lifetimes']\n",
      "offsets position: [97]\n",
      "Position of the sentence: [2]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 24: plotlin (stemmed form)\n",
      "--> Main word: ['plotlines']\n",
      "offsets position: [116]\n",
      "Position of the sentence: [2]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 26: histori (stemmed form)\n",
      "--> Main word: ['history']\n",
      "offsets position: [122]\n",
      "Position of the sentence: [3]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 30: studi (stemmed form)\n",
      "--> Main word: ['studies']\n",
      "offsets position: [133]\n",
      "Position of the sentence: [3]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 32: complaint (stemmed form)\n",
      "--> Main word: ['complaints']\n",
      "offsets position: [147]\n",
      "Position of the sentence: [4]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 33: white (stemmed form)\n",
      "--> Main word: ['whiteness']\n",
      "offsets position: [150]\n",
      "Position of the sentence: [4]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 34: straight (stemmed form)\n",
      "--> Main word: ['straightness']\n",
      "offsets position: [152]\n",
      "Position of the sentence: [4]\n",
      "Parts of speech: [['NOUN']]\n",
      "candidate 35: reader (stemmed form)\n",
      "--> Main word: ['readers']\n",
      "offsets position: [161]\n",
      "Position of the sentence: [4]\n",
      "Parts of speech: [['NOUN']]\n"
     ]
    }
   ],
   "source": [
    "word_pos.candidate_selection()\n",
    "for i, candidate in enumerate(word_pos.candidates):\n",
    "  ################### We are extracting words only belongs to the category of Noun###################################\n",
    "  ################### Please change to other tag if wanted###########################################################\n",
    "     if word_pos.candidates[candidate].pos_patterns==[['NOUN']]:\n",
    "            print(\"candidate {}: {} (stemmed form)\".format(i, candidate))\n",
    "            print(\"--> Main word:\", [ \" \".join(u) for u in word_pos.candidates[candidate].surface_forms])\n",
    "            print(\"offsets position:\", word_pos.candidates[candidate].offsets)\n",
    "            print(\"Position of the sentence:\", word_pos.candidates[candidate].sentence_ids)\n",
    "            print(\"Parts of speech:\", word_pos.candidates[candidate].pos_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hRyGKIwFeAX",
    "outputId": "2a8d6f13-32f9-415a-8fa1-6daca0dc9164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roald dahl: 0.053365895427066455\n",
      "harri potter fan fiction: 0.02381784384638097\n",
      "potter univers: 0.018110622110733828\n",
      "enid blyton: 0.05973200914860123\n",
      "acr: 0.020275153821384992\n",
      "anti-semit statement: 0.029881480149339768\n",
      "book: 0.05101446844457456\n",
      "british private-school system: 0.025654518499956227\n",
      "casual racism: 0.027802583421211325\n",
      "complaint: 0.023171750478132604\n",
      "consum revolt: 0.02717780073956239\n",
      "discomfort: 0.018264169442154976\n",
      "elit: 0.02103002901755053\n",
      "entitl: 0.03398737167502929\n",
      "fan-creat relationship: 0.02900616224123117\n",
      "fave: 0.03090460811530189\n",
      "histori: 0.028885420184343667\n",
      "institut: 0.023831607314713014\n",
      "internet: 0.027725879990547226\n",
      "intimaci: 0.03190421478014803\n",
      "languag: 0.023895083838762696\n",
      "lifetim: 0.030739159434519\n",
      "millenni audienc: 0.0208573063245724\n",
      "mixtur: 0.03157142483914488\n",
      "modern: 0.0276257757598118\n",
      "nostalg set: 0.022337697483981612\n",
      "person opinion: 0.02891564782043814\n",
      "plotlin: 0.028722550306488268\n",
      "problemat: 0.031064875318048756\n",
      "reader: 0.022317360527737733\n",
      "stori: 0.01929285944657074\n",
      "straight: 0.0270088925459717\n",
      "studi: 0.031875460063690984\n",
      "unguard tweet: 0.020990350346647344\n",
      "white: 0.027241967095649553\n"
     ]
    }
   ],
   "source": [
    "# Topic Rank\n",
    "#The main goal for this section is the keywords with which we are forming gapped exercises can be considered important words or not. \n",
    "#If all the words are selected by the following process (even with low probability scores then we can consider our evaluation is perfectly right \n",
    "#because this process does not select illogical or unreasonable words in the sentences).\n",
    "\n",
    "word_pos.candidate_weighting()\n",
    "for candidate, weight in word_pos.weights.items():\n",
    "    print('{}: {}'.format(candidate, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n6prG60Ugqd"
   },
   "source": [
    "we have included many conditions while selecting keyphrases like extraction of only the common words given by our Nltk position tags and key phrase extractors using PKE and also sentence conditioning like if we have 2 keyphrases adjacently then we are skipping last 2 and giving importance to the first word. \n",
    "\n",
    "we are not getting high probabilty score by topic because topic rank gives high importance to proper nouns first and then other words.Our main goal is not about getting high score but it is about whether the words which are extracted as keywords by our process are getting selected by the topic rank or not. If this is shown then our process is doing fine when we consider overall logic we provided because topic rank does not select illogical or unreasonable words as important topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl70m590ETgN"
   },
   "source": [
    "## Sample generation of questions from pre-trained model using the sentences generated from the above process.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTkyEuDc3YkX",
    "outputId": "4a9fa0d4-4911-4305-ba13-2c63b179e99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "## Please note that we are generating questions from a pre-trained model from github.\n",
    "\n",
    "\n",
    "!pip install --quiet git+https://github.com/huggingface/transformers.git@5c00918681d6b4027701eb46cea8f795da0d4064\n",
    "!pip install --quiet sentencepiece==0.1.95\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "main_model='ramsrigouthamg/t5_squad_v1' #we are downloading this pre-trained model from hugging face\n",
    "question_gen = T5ForConditionalGeneration.from_pretrained(main_model)\n",
    "question_tokenizer = T5Tokenizer.from_pretrained(main_model)\n",
    "def get_question(sentence,answer,mdl,tknizer):\n",
    "        text = \"context: {} answer: {}\".format(sentence,answer)\n",
    "        print (text)\n",
    "        max_len = 300\n",
    "        encoding = tknizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "        outs = mdl.generate(input_ids=input_ids,attention_mask=attention_mask,early_stopping=True,num_beams=5,num_return_sequences=1,no_repeat_ngram_size=2,max_length=300)\n",
    "        dec = [tknizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
    "        Question = dec[0].replace(\"question:\",\"\")\n",
    "        Question= Question.strip()\n",
    "        return Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGF0o0W5HDSx"
   },
   "source": [
    "## Extracting questions based on the keysentences we generated from the above process.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2q_0CPlnnxl",
    "outputId": "fe190cb4-d265-41a0-b544-244de3318322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: The acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism). answer: stories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:965: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1830: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Rowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the internet, their fave is problematic. answer: internet\n",
      "context: This fan-creator relationship is a peculiarly modern one, a mixture of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines. answer: mixture\n",
      "context: (Dahl had a history of making anti-Semitic statements, and Blyton’s books are studies in casual racism.) answer: history\n",
      "context: They didn’t live long enough to see complaints about the whiteness and straightness of their books, or to upset their readers with unguarded tweets. answer: straightness\n"
     ]
    }
   ],
   "source": [
    "questions=[]\n",
    "for words in fill_in_the_blanks['keys']:\n",
    "    for sent in final_sentences:\n",
    "        if words in sent:\n",
    "            ques = get_question(sent,words,question_gen,question_tokenizer)\n",
    "            questions.append(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKVDV-MFEMNv",
    "outputId": "7801a66f-756c-4929-8573-736ae1a9fb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mRead the following paragraph carefully and answer the provided questions : \n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[31mThe acres of Harry Potter fan fiction have allowed its Millennial audience to rewrite the stories to fit their own values, easing their discomfort (while still luxuriating in the nostalgic setting of the British private-school system, an institution designed to perpetuate elitism).\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[31mRowling’s newer work in the Potter universe reminds them, however, that none of this is canon—that, in the language of the internet, their fave is problematic.\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[31mThis fan-creator relationship is a peculiarly modern one, a mixture of entitlement and intimacy: In their lifetimes, Enid Blyton and Roald Dahl were not troubled by consumer revolts over their personal opinions or their plotlines.\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[31m(Dahl had a history of making anti-Semitic statements, and Blyton’s books are studies in casual racism.)\u001b[0m \n",
      "\n",
      "\u001b[1m\u001b[31mThey didn’t live long enough to see complaints about the whiteness and straightness of their books, or to upset their readers with unguarded tweets.\u001b[0m \n",
      "\n",
      "What has Harry Potter fan fiction allowed Millennials to rewrite? \n",
      "\n",
      "In what language is Rowling's work problematic? \n",
      "\n",
      "How modern is the fan-creator relationship? \n",
      "\n",
      "What was Dahl's background in anti-Semitism? \n",
      "\n",
      "Along with whiteness, what was a complaint of the authors? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(colored('Read the following paragraph carefully and answer the provided questions : \\n','blue',attrs=['bold']))\n",
    "wrapped=[print(colored((sentences),'red',attrs=['bold']),'\\n')  for sentences in sent_list]\n",
    "for ques in questions:\n",
    "    print(ques,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CDaXrDgrd3sG"
   },
   "outputs": [],
   "source": [
    "#These are the same sample input texts taken from other notebook. We are evaluating both our models based on BLEU score. \n",
    "text = [\"Eagle is the bravest bird\",\"Dog is always a best friend to human beings\",\"English is the world's most popular language\",\"Chess is invented by the indians\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quaw2vh9d4Ud"
   },
   "source": [
    "# BLEU SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QixC2paUeA4t",
    "outputId": "36bb869f-6073-40e5-aba4-c31b01ddc7bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: Eagle is the bravest bird answer: Eagle\n",
      "context: Dog is always a best friend to human beings answer: Dog\n",
      "context: English is the world's most popular language answer: English\n",
      "context: Chess is invented by the indians answer: Chess\n"
     ]
    }
   ],
   "source": [
    "questions1=[]\n",
    "words=['Eagle','Dog','English','Chess']\n",
    "for words in words:\n",
    "        for sent in text:\n",
    "                 if words in sent:\n",
    "                     ques = get_question(sent,words,question_gen,question_tokenizer)\n",
    "                \n",
    "                     questions1.append(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJ7SAMyIf8i1",
    "outputId": "513cabcd-c8f6-47cf-8012-0e60173d7c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the bravest bird?',\n",
       " 'Who is always a best friend to humans?',\n",
       " 'What is the most popular language in the world?',\n",
       " 'What game was invented by the indians?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcn_YjYogaTM"
   },
   "source": [
    "# Generated questions from both of the models using same text to evaluate them using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KSLKAFd7gYgs"
   },
   "outputs": [],
   "source": [
    "### Iam passing all the questions we generated from our BERT model as reference parameters.\n",
    "### questions from the enhanced model present in this notebook as candiadate parameters.\n",
    "### Indirectly we are evaluating the performance of the model which is fine-tuned.\n",
    "Q11= [['What is the bravest bird?'],['Which animal is always a best friend to humans?'],['What is the most popular language in the world?'],['Who invented chess?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "se651Qecg4hq"
   },
   "outputs": [],
   "source": [
    "Q22=[['What is the bravest bird?'],['Who is always a best friend to humans?'],['What is the most popular language in the world?'],['What game was invented by the indians?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQuSXMeEhtJr",
    "outputId": "d8a459a1-5042-46c3-90a1-2acef15cfe93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU SCORE of first model question generation ['What is the bravest bird?'] -- second model question generation [What is the bravest bird?] is 1.0\n",
      "BLEU SCORE of first model question generation ['Which animal is always a best friend to humans?'] -- second model question generation [Who is always a best friend to humans?] is 0.7598356856515925\n",
      "BLEU SCORE of first model question generation ['What is the most popular language in the world?'] -- second model question generation [What is the most popular language in the world?] is 1.0\n",
      "BLEU SCORE of first model question generation ['Who invented chess?'] -- second model question generation [What game was invented by the indians?] is 5.635809992474887e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "import  nltk.translate.bleu_score as bleu\n",
    "import numpy\n",
    "if (len(Q11))==(len(Q22)):\n",
    "            i=0\n",
    "            for ques1 in Q11:\n",
    "                   for ques2 in Q22:\n",
    "                         if (Q11.index(ques1))==(Q22.index(ques2)):\n",
    "                                ref=[nltk.word_tokenize(line) for line in Q22[i]]\n",
    "                                cand=nltk.word_tokenize(Q11[i][0])\n",
    "                                print('BLEU SCORE of first model question generation {} -- second model question generation [{}] is {}'.format(Q11[i],Q22[i][0],bleu.sentence_bleu(ref,cand)))\n",
    "                                i=i+1   \n",
    "\n",
    "#Bleu score tells us how much difference we are making between two models according to question texts we generated."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
